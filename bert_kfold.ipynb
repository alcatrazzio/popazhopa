{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4fb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7254dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9239fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']] = train[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']].fillna('Пусто')\n",
    "test[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']] = test[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']].fillna('Пусто')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f74a8ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['PriceDiscounted'] = 2**(train.PriceDiscounted/69.6606)\n",
    "test['PriceDiscounted'] = 2**(test.PriceDiscounted/69.6606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a0bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = ('Тип товара:\\n' + train['CommercialTypeName4'] + '\\n\\nНазвание товара:\\n' + train['name_rus'] + '\\n\\nБренд:\\n' + train['brand_name'] + '\\n\\nОписание:\\n' + train['description'] + '\\n\\nЦена:\\n' + train['PriceDiscounted'].round().map(int).map(str) + ' рублей').values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fbb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.resolution.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa8e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тип товара:\n",
      "Пылесборник\n",
      "\n",
      "Название товара:\n",
      "Мешки для пылесоса PHILIPS TRIATLON, синтетические, многослойные, тип: HR 6947\n",
      "\n",
      "Бренд:\n",
      "ACTRUM\n",
      "\n",
      "Описание:\n",
      "Мешки пылесборники для пылесоса PHILIPS, 10 шт., синтетические, многослойные, бренд: ACTRUM, арт. AK-10/10, тип оригинального мешка: HR 6947.Подходят для пылесосов:PHILIPS: HR6955, HR6947, HR6888, HR6844 TRIATHLON, HR6843 TRIATHLON, HR6842 TRIATHLON, HR6841 TRIATHLON, HR6840 TRIATHLON, HR6839 TRIATHLON, HR6838 TRIATHLON, HR6837 TRIATHLON, HR6836 TRIATHLON, HR6835 TRIATHLON, HR6834 TRIATHLON, HR6833 TRIATHLON, HR6832 TRIATHLON, HR6831 TRIATHLON, HR6830 TRIATHLON, HR6829 TRIATHLON, HR6828 TRIATHLON, HR6827 TRIATHLON, HR6826 TRIATHLON, HR6825 TRIATHLON, HR6824 TRIATHLON, HR6823 TRIATHLON, HR6822 TRIATHLON, HR6821 TRIATHLON, HR6820 TRIATHLON, HR6819 TRIATHLON, HR6818 TRIATHLON, HR6817 TRIATHLON, HR6816 TRIATHLON, HR6815 TRIATHLON, HR6814 - HR6845 TRIATHLON, FC6844 TRIATHLON, FC6843 TRIATHLON, FC6842 TRIATHLON, FC6841 - FC6845 TRIATHLONОдноразовые мешки-пылесборники ACTRUM изгот\n",
      "\n",
      "Цена:\n",
      "944 рублей\n"
     ]
    }
   ],
   "source": [
    "print(all_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3f32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sergeyzh/BERTA\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646da100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5cac3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sergeyzh/BERTA and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2  # Бинарная классификация\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f8f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c2dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(preds, labels):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return f1_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d38fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return np.mean(losses), correct_predictions.double() / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9900ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            all_preds.extend(logits.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    return np.mean(losses), accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dad68401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, device, max_length=256):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probabilities, dim=1).cpu().numpy()[0]\n",
    "        confidence = probabilities.cpu().numpy()[0][prediction]\n",
    "    \n",
    "    return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97dcf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']] = test[['CommercialTypeName4', 'description', 'brand_name', 'name_rus']].fillna('Пусто')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe1cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab5e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts_test = ('Тип товара:\\n' + test['CommercialTypeName4'] + '\\n\\nНазвание товара:\\n' + test['name_rus'] + '\\n\\nБренд:\\n' + test['brand_name'] + '\\n\\nОписание:\\n' + test['description'] + '\\n\\nЦена:\\n' + test['PriceDiscounted'].round().map(int).map(str) + ' рублей').values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b293b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 1\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [28:43<00:00,  1.61it/s] \n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0905, Train Acc: 0.9667\n",
      "Val Loss: 0.0752, Val F1: 0.7790\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [28:02<00:00,  1.65it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0635, Train Acc: 0.9766\n",
      "Val Loss: 0.0712, Val F1: 0.8048\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [28:02<00:00,  1.65it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0522, Train Acc: 0.9811\n",
      "Val Loss: 0.0705, Val F1: 0.8115\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:30<00:00, 151.27it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 2\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:11<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0653, Train Acc: 0.9763\n",
      "Val Loss: 0.0545, Val F1: 0.8447\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:04<00:00,  1.71it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0482, Train Acc: 0.9823\n",
      "Val Loss: 0.0542, Val F1: 0.8457\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:03<00:00,  1.71it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0392, Train Acc: 0.9857\n",
      "Val Loss: 0.0578, Val F1: 0.8530\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:28<00:00, 153.66it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 3\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:01<00:00,  1.71it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0543, Train Acc: 0.9803\n",
      "Val Loss: 0.0424, Val F1: 0.8807\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:03<00:00,  1.71it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0385, Train Acc: 0.9858\n",
      "Val Loss: 0.0415, Val F1: 0.8889\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:10<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0304, Train Acc: 0.9889\n",
      "Val Loss: 0.0427, Val F1: 0.8886\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:27<00:00, 154.18it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 4\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:07<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0456, Train Acc: 0.9835\n",
      "Val Loss: 0.0357, Val F1: 0.9040\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:11<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0310, Train Acc: 0.9886\n",
      "Val Loss: 0.0360, Val F1: 0.9080\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:45<00:00,  1.67it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0243, Train Acc: 0.9909\n",
      "Val Loss: 0.0377, Val F1: 0.9094\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:30<00:00, 151.49it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 5\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:33<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0397, Train Acc: 0.9860\n",
      "Val Loss: 0.0276, Val F1: 0.9273\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:30<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0265, Train Acc: 0.9904\n",
      "Val Loss: 0.0279, Val F1: 0.9291\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:30<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0200, Train Acc: 0.9924\n",
      "Val Loss: 0.0291, Val F1: 0.9270\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:29<00:00, 152.00it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 6\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:32<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0346, Train Acc: 0.9874\n",
      "Val Loss: 0.0258, Val F1: 0.9262\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:30<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0221, Train Acc: 0.9918\n",
      "Val Loss: 0.0274, Val F1: 0.9275\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:30<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0167, Train Acc: 0.9938\n",
      "Val Loss: 0.0305, Val F1: 0.9284\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:30<00:00, 151.39it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 7\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:29<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0313, Train Acc: 0.9892\n",
      "Val Loss: 0.0178, Val F1: 0.9468\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:27<00:00,  1.68it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0200, Train Acc: 0.9928\n",
      "Val Loss: 0.0195, Val F1: 0.9445\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:26<00:00,  1.69it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0149, Train Acc: 0.9943\n",
      "Val Loss: 0.0208, Val F1: 0.9465\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:29<00:00, 152.43it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 8\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:11<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0283, Train Acc: 0.9900\n",
      "Val Loss: 0.0182, Val F1: 0.9453\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:13<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0176, Train Acc: 0.9936\n",
      "Val Loss: 0.0181, Val F1: 0.9507\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:10<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0123, Train Acc: 0.9953\n",
      "Val Loss: 0.0215, Val F1: 0.9504\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:29<00:00, 152.63it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 9\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:13<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0253, Train Acc: 0.9911\n",
      "Val Loss: 0.0132, Val F1: 0.9579\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:14<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0158, Train Acc: 0.9941\n",
      "Val Loss: 0.0130, Val F1: 0.9642\n",
      "Model saved!\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:10<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0113, Train Acc: 0.9957\n",
      "Val Loss: 0.0141, Val F1: 0.9648\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:28<00:00, 152.77it/s]\n",
      "c:\\Projects\\ozon cup 2025\\.venv\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Fold: 10\n",
      "--------------------------------------------------\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:12<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:07<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0240, Train Acc: 0.9916\n",
      "Val Loss: 0.0141, Val F1: 0.9583\n",
      "Model saved!\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:08<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0139, Train Acc: 0.9949\n",
      "Val Loss: 0.0138, Val F1: 0.9557\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2774/2774 [27:13<00:00,  1.70it/s]\n",
      "Validation: 100%|██████████| 309/309 [01:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9962\n",
      "Val Loss: 0.0173, Val F1: 0.9593\n",
      "Model saved!\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22760/22760 [02:28<00:00, 153.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "preds_test = np.zeros(len(test))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(all_texts, labels)):\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Fold: {fold+1}\")\n",
    "    print(\"-\" * 50)\n",
    "    train_texts, val_texts = pd.Series(all_texts)[train_idx].tolist(),  pd.Series(all_texts)[test_idx].tolist()\n",
    "    train_labels, val_labels = pd.Series(labels)[train_idx].tolist(), pd.Series(labels)[test_idx].tolist()\n",
    "\n",
    "    train_dataset = TextClassificationDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "    val_dataset = TextClassificationDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Основной цикл обучения\n",
    "    best_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_preds, val_labels = eval_model(\n",
    "            model, val_loader, device\n",
    "        )\n",
    "        \n",
    "        val_f1 = calculate_f1(val_preds, val_labels)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Сохранение лучшей модели\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), f'berta_{fold+1}.pth')\n",
    "            print(\"Model saved!\")\n",
    "    \n",
    "    model.save_pretrained(f\"./berta_binary_classifier_{fold}\")\n",
    "    tokenizer.save_pretrained(f\"./berta_binary_classifier_{fold}\")\n",
    "    print(\"Model and tokenizer saved!\")\n",
    "\n",
    "    preds = [predict(text, model, tokenizer, device) for text in tqdm(all_texts_test)]\n",
    "    \n",
    "    probas = []\n",
    "\n",
    "    for pred, conf in preds:\n",
    "        if pred == 0:\n",
    "            probas.append(1 - conf)\n",
    "        else:\n",
    "            probas.append(conf)\n",
    "    probas = np.array(probas)\n",
    "\n",
    "    preds_test += probas / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efe01ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_probs = pd.read_csv('multimodal_proba.csv').proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21821d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>308655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  prediction\n",
       "0   17384           0\n",
       "1  260316           0\n",
       "2   10610           0\n",
       "3  205236           0\n",
       "4  308655           0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'], \n",
    "    # 'prediction': (preds_test * 0.4 + multimodal_probs * 0.6 >= 0.6).astype(int) best\n",
    "    # 'prediction': (np.array([max([a, b]) for a, b in zip(preds_test, multimodal_probs)]) >= 0.8).astype(int)\n",
    "    # 'prediction': (preds_test >= 0.7).astype(int)\n",
    "    'prediction': (preds_test * 0.4 + multimodal_probs * 0.6 >= 0.6).astype(int)\n",
    "\n",
    "})\n",
    "submission.to_csv('berta_kfolds.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddc6932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.044200351493848856)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83799ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
