{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d919fef",
   "metadata": {
    "papermill": {
     "duration": 8.826578,
     "end_time": "2025-08-15T15:28:57.474471",
     "exception": true,
     "start_time": "2025-08-15T15:28:48.647893",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.auto import tqdm\n",
    "from helpers import IOU, blend_intervals\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40de4691",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde8a69-5a2f-4374-9bb3-862332865401",
   "metadata": {},
   "source": [
    "### Указываем какие лора адаптеры нам нужны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f1b671-a988-4bbc-bb30-5f21d4dde608",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_reqs = [\n",
    "    LoRARequest(\n",
    "        lora_name=\"peft_for_fining_mistakes\",\n",
    "        lora_int_id=i + 1,\n",
    "        lora_path=f\"./outputs/qwen-sft-fold-{i}\",\n",
    "        base_model_name=\"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    ) for i in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b27a84-83e9-4262-9f9b-c9e68fda8fdb",
   "metadata": {},
   "source": [
    "### Подгружаем ту же модельку через VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30333d17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 15:48:52 [utils.py:233] non-default args: {'trust_remote_code': True, 'seed': 56, 'max_model_len': 81920, 'max_num_seqs': 64, 'disable_log_stats': True, 'enable_lora': True, 'model': 'Qwen/Qwen3-4B-Instruct-2507'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 15:48:53 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 15:48:54 [model.py:1510] Using max model len 81920\n",
      "INFO 10-22 15:48:57 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 10-22 15:48:57 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:48:58 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:48:58 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=81920, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=56, served_model_name=Qwen/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":128,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:01 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m WARNING 10-22 15:49:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:01 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-4B-Instruct-2507...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:02 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:02 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:02 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69151df3b1b74f988b617fd1c65ed8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:04 [default_loader.py:267] Loading weights took 1.74 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:04 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:05 [gpu_model_runner.py:2653] Model loading took 7.6684 GiB and 2.774170 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:17 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/72c553a21c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:17 [backends.py:559] Dynamo bytecode transform time: 10.26 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:22 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.255 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:24 [monitor.py:34] torch.compile takes 10.26 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:26 [gpu_worker.py:298] Available KV cache memory: 19.84 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:26 [kv_cache_utils.py:1087] GPU KV cache size: 144,432 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:26 [kv_cache_utils.py:1091] Maximum concurrency for 81,920 tokens per request: 1.76x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.95it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:32 [gpu_model_runner.py:3480] Graph capturing finished in 5 secs, took 0.21 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=37735)\u001b[0;0m INFO 10-22 15:49:32 [core.py:210] init engine (profile, create kv cache, warmup model) took 26.47 seconds\n",
      "INFO 10-22 15:49:33 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    max_num_seqs=64,\n",
    "    max_model_len=81920,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    "    max_lora_rank=16,\n",
    "    seed=56,\n",
    "    enable_lora=True\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "937c2866",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.6,              \n",
    "    n=32,\n",
    "    top_p=0.95,                    \n",
    "    min_p=0.0, \n",
    "    top_k=20,\n",
    "    skip_special_tokens=True,\n",
    "    max_tokens=81920,\n",
    "    seed=56,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4bdca8-9841-478a-a7aa-dc3360661bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTIONS = (\n",
    "    \"You tag mistakes in student math solutions.\\n\"\n",
    "    \"- Output must be EXACTLY the student's solution text, with <mistake>...</mistake> tags around mistakes.\\n\"\n",
    "    \"- Do NOT add or remove any other text, lines, or spaces.\\n\"\n",
    "    \"- Do NOT add commentary or explanations.\\n\"\n",
    ")\n",
    "\n",
    "INFER_INSTRUCTIONS = SYSTEM_INSTRUCTIONS\n",
    "\n",
    "def build_prompt(task: str, solution: str) -> str:\n",
    "    return (\n",
    "        f\"{INFER_INSTRUCTIONS}\\n\\n\"\n",
    "        f\"Problem:\\n{task}\\n\\n\"\n",
    "        f\"Student solution:\\n{solution}\\n\\n\"\n",
    "        f\"Tagged solution:\\n\"\n",
    "    )\n",
    "\n",
    "def get_answer_from_model(rows, lora_req):\n",
    "    messages = [[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_INSTRUCTIONS,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": build_prompt(row['task'], row['solution']),\n",
    "        }\n",
    "    ] for row in rows]\n",
    "\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        for message in messages\n",
    "    ]\n",
    "    result = llm.generate(prompts=list_of_texts, sampling_params=sampling_params, lora_request=lora_req)\n",
    "    result = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eaf4e9-0568-4b1b-b00e-a1346dba7e10",
   "metadata": {},
   "source": [
    "### Запускае "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34dc101-a817-4df6-a2ab-2d12f79159ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_private_new_without_answer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e167c7-a723-414f-9cab-f0a753761b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_rows = [x for _, x in test_data.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93f05f3e-af23-4c0b-95f9-6edcc8fab07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f1b3ac38f240b781a9b90e5dd5dbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-22 15:53:05 [processor.py:215] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43aebbb57e504795aec5004b18eabfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14400 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1dd318793c47a9ba5a653879396ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c885c9d254ecc93082cb3617ab3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14400 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d69497076541098a9544c30ad4d602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960dc6f93345400da985f6021dd9c61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14400 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d3499d842b4c5a86e5c1ef4c8dfaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e104b68676df4b40b06283ce9758c18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14400 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a6b8bc4f4143aba7bebd0ed7497b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e55485fd5bf48f4b90143463138dd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/14400 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_test_i_model = []\n",
    "for lora_req in lora_reqs:\n",
    "    results_test = get_answer_from_model(test_data_rows, lora_req=lora_req)\n",
    "    preds_test_i_model.append(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0da833-e877-49c5-8dbb-af65041618ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTAKE_PATTERN = re.compile(r\"<mistake>(.*?)</mistake>\", flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def get_intervals(x):\n",
    "    spans = []\n",
    "    bias = 19\n",
    "    bias_pre = 0\n",
    "    for m in re.finditer(MISTAKE_PATTERN, x):\n",
    "        spans.append([m.span()[0] - bias_pre, m.span()[1] - bias])\n",
    "        bias_pre += 19\n",
    "        bias += 19\n",
    "    return spans\n",
    "\n",
    "def process_result(x, sol):\n",
    "    first = get_intervals(x.outputs[0].text)\n",
    "    intervals_all = []\n",
    "    intervals_all.append(first)\n",
    "    for i in range(1,32):\n",
    "        second = get_intervals(x.outputs[i].text)\n",
    "        intervals_all.append(second)\n",
    "        first = blend_intervals(first, second)\n",
    "        \n",
    "    intervals_all_dict = []\n",
    "    for span in first:\n",
    "        intervals_dict = {}\n",
    "        intervals_dict['text'] = sol[span[0]:span[1]]\n",
    "        intervals_dict['start'] = span[0]\n",
    "        intervals_dict['end'] = span[1]\n",
    "        intervals_all_dict.append(intervals_dict)\n",
    "    return intervals_all_dict, intervals_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ff1cde5-9f50-4d31-a8b8-7cff2cc02691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ious = []\n",
    "fold_prediction = []\n",
    "for fold in range(5):\n",
    "    spans_predictions = []\n",
    "    for item, sol in zip(preds_test_i_model[fold], test_data_rows):\n",
    "        spans_predictions.append(process_result(item, sol['solution'])[1])\n",
    "    fold_prediction.append(spans_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd72a278-c854-4ff6-b21d-0f1fca8c22fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fold_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eff4fa6d-7c70-4b5f-a10b-1d3f411abe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds = []\n",
    "for i in range(5):\n",
    "    all_samples = fold_prediction[i]\n",
    "    for j in range(450):\n",
    "        all_ns = all_samples[j]\n",
    "        for n in range(32):\n",
    "            df_all_preds.append({'fold' : i, 'n' : n, 'sample' : j, 'answer' : all_ns[n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "248cbe6d-3a51-4b63-a06b-dc13568dcb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_all_preds).to_csv('llm_predictions_private.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8df908-4ac0-4e66-87bb-6f7db8fec6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "isSourceIdPinned": false,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8460824,
     "sourceId": 13342246,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 215.813362,
   "end_time": "2025-08-15T15:28:58.694867",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-15T15:25:22.881505",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
