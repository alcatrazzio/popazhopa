{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81643e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %mkdir notebooks\n",
    "# %cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79074bba",
   "metadata": {},
   "source": [
    "## **Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e240a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed=42):\n",
    "    # Устанавливаем seed для встроенного генератора Python\n",
    "    random.seed(seed)\n",
    "    # Устанавливаем seed для хэш-функции Python (опция для контроля поведения хэшей)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # Устанавливаем seed для NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Устанавливаем seed для PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    # Устанавливаем seed для генератора на CUDA\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # Отключаем недетерминированное поведение в алгоритмах CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b143fe9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d03e6",
   "metadata": {},
   "source": [
    "### **Initial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed67583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/ml_ozon_сounterfeit_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/ml_ozon_сounterfeit_test.csv\")\n",
    "new_test_df = pd.read_csv(\"../data/ml_ozon_сounterfeit_new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[\"resolution\"]\n",
    "test_labels = pd.read_csv(\"../submissions/best.csv\")['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df.drop(columns=[\"resolution\"]), test_df, new_test_df])\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]\n",
    "new_test_size = new_test_df.shape[0]\n",
    "del train_df, test_df, new_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cad37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [\n",
    "    'rating_1_count',\n",
    "    'rating_2_count',\n",
    "    'rating_3_count',\n",
    "    'rating_4_count',\n",
    "    'rating_5_count',\n",
    "    'comments_published_count',\n",
    "    'photos_published_count',\n",
    "    'videos_published_count',\n",
    "    'PriceDiscounted',\n",
    "    'item_time_alive',\n",
    "    'item_count_fake_returns7',\n",
    "    'item_count_fake_returns30',\n",
    "    'item_count_fake_returns90',\n",
    "    'item_count_sales7',\n",
    "    'item_count_sales30',\n",
    "    'item_count_sales90',\n",
    "    'item_count_returns7',\n",
    "    'item_count_returns30',\n",
    "    'item_count_returns90',\n",
    "    'GmvTotal7',\n",
    "    'GmvTotal30',\n",
    "    'GmvTotal90',\n",
    "    'ExemplarAcceptedCountTotal7',\n",
    "    'ExemplarAcceptedCountTotal30',\n",
    "    'ExemplarAcceptedCountTotal90',\n",
    "    'OrderAcceptedCountTotal7',\n",
    "    'OrderAcceptedCountTotal30',\n",
    "    'OrderAcceptedCountTotal90',\n",
    "    'ExemplarReturnedCountTotal7',\n",
    "    'ExemplarReturnedCountTotal30',\n",
    "    'ExemplarReturnedCountTotal90',\n",
    "    'ExemplarReturnedValueTotal7',\n",
    "    'ExemplarReturnedValueTotal30',\n",
    "    'ExemplarReturnedValueTotal90',\n",
    "    'ItemVarietyCount',\n",
    "    'ItemAvailableCount',\n",
    "    'seller_time_alive',\n",
    "]\n",
    "df[num_features] = df[num_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df[num_features] = scaler.fit_transform(df[num_features]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391add",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_raw_features = ['SellerID', 'CommercialTypeName4', 'brand_name']\n",
    "cat_features = []\n",
    "for col in cat_raw_features:\n",
    "    df[f'{col}_enc'] = LabelEncoder().fit_transform(df[col])\n",
    "    cat_features.append(f'{col}_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6eef2e",
   "metadata": {},
   "source": [
    "### Text&Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ee1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = np.load('../data/embeddings/image_clip_large_half_embeddings.npy')\n",
    "text_embeddings = np.load('../data/embeddings/text_rubert_tiny2_half_embeddings.npy')\n",
    "\n",
    "df['image_embedding'] = list(image_embeddings)\n",
    "df['text_embedding'] = list(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19839719",
   "metadata": {},
   "source": [
    "### **Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3da896",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, new_test_df = df[:train_size], df[train_size:train_size + test_size], df[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "train_test_labels = pd.concat([train_labels, test_labels], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d085aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "splits = skf.split(train_test_df, train_test_labels, groups=train_test_df['SellerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2508c5",
   "metadata": {},
   "source": [
    "### **Class Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c367770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, labels=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.labels = np.array(labels) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Извлекаем эмбеддинги\n",
    "        image_data = row['image_embedding']\n",
    "        text_data = row['text_embedding']\n",
    "\n",
    "        # Числовые признаки\n",
    "        num_data = torch.tensor(row[num_features].values.astype(float))\n",
    "\n",
    "        # Категориальные признаки\n",
    "        cat_data = torch.tensor(row[cat_features].values.astype(int), dtype=torch.long)\n",
    "\n",
    "        # Собираем словарь модальностей\n",
    "        sample = {\n",
    "            'image': image_data,\n",
    "            'text': text_data,\n",
    "            'cat': cat_data,\n",
    "            'num': num_data,\n",
    "        }\n",
    "\n",
    "        # Добавляем label, если есть\n",
    "        if self.labels is not None:\n",
    "            sample['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc837e",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a728724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, cat_embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.cat_embed = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=df[feature].nunique() + 1, embedding_dim=cat_embed_dim)\n",
    "            for feature in cat_features\n",
    "        ])\n",
    "        self.cat_norm = nn.LayerNorm(cat_embed_dim * len(cat_features))\n",
    "        self.num_layers = nn.Sequential(\n",
    "            nn.Linear(len(num_features), hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(624, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(1024, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1152, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, sample):\n",
    "        cat_embeds = torch.cat([\n",
    "            self.cat_embed[i](sample['cat'][:, i]) for i in range(len(self.cat_embed))\n",
    "        ], dim=1)\n",
    "        cat_embeds = self.cat_norm(cat_embeds)\n",
    "        num_outputs = self.num_layers(sample['num'].to(torch.float32))\n",
    "        text_embeds = self.text_proj(sample['text'].to(torch.float32))\n",
    "        image_embeds = self.image_proj(sample['image'].to(torch.float32))\n",
    "        concat = torch.cat([text_embeds, image_embeds, cat_embeds, num_outputs], dim=1)\n",
    "        logits = self.fc(concat)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5c8f1",
   "metadata": {},
   "source": [
    "### *Train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_models = []\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for split_idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "    split_train_df, split_valid_df = train_test_df.iloc[train_idx], train_test_df.iloc[valid_idx]\n",
    "    split_train_labels, split_valid_labels = train_test_labels.iloc[train_idx], train_test_labels.iloc[valid_idx]\n",
    "\n",
    "    train_set = MultimodalDataset(split_train_df, split_train_labels)\n",
    "    valid_set = MultimodalDataset(split_valid_df, split_valid_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = MultimodalModel().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    num_epochs = 3\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        y_true, y_pred = [], []\n",
    "        model.train()\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            batch['cat'] = batch['cat'].to(device)\n",
    "            batch['num'] = batch['num'].to(device)\n",
    "            batch['text'] = batch['text'].to(device)\n",
    "            batch['image'] = batch['image'].to(device)\n",
    "            batch['labels'] = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs, batch['labels'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_true.extend(batch['labels'].cpu().tolist())\n",
    "            y_pred.extend(outputs.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item(), score=f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(valid_loader, desc=\"Validating\")\n",
    "            for i, batch in enumerate(progress_bar):\n",
    "                batch['cat'] = batch['cat'].to(device)\n",
    "                batch['num'] = batch['num'].to(device)\n",
    "                batch['text'] = batch['text'].to(device)\n",
    "                batch['image'] = batch['image'].to(device)\n",
    "                batch['labels'] = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(batch)\n",
    "                loss = loss_fn(outputs, batch['labels'])\n",
    "\n",
    "                y_true.extend(batch['labels'].cpu().tolist())\n",
    "                y_pred.extend(outputs.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    progress_bar.set_postfix(loss=loss.item(), score=f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "        print(f\"Split {split_idx + 1} - Epoch {epoch} - Score: {f1_score(y_true, y_pred, average='macro')}\\n\")\n",
    "            \n",
    "    training_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(training_models):\n",
    "    torch.save(model.state_dict(), f\"../models/multimodal_{idx + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1290ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_models = [MultimodalModel() for _ in range(n_splits)]\n",
    "# for idx, model in enumerate(training_models):\n",
    "#     model.load_state_dict(torch.load(f\"../models/multimodal_{idx + 1}.pth\"))\n",
    "#     model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f1cfb",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_set = MultimodalDataset(new_test_df)\n",
    "new_test_loader = DataLoader(new_test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "probabilities = []\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(new_test_loader, desc=\"Testing\")\n",
    "    for batch in progress_bar:\n",
    "        batch['cat'] = batch['cat'].to(device)\n",
    "        batch['num'] = batch['num'].to(device)\n",
    "        batch['text'] = batch['text'].to(device)\n",
    "        batch['image'] = batch['image'].to(device)\n",
    "\n",
    "        batch_probabilities = [\n",
    "            torch.softmax(model(batch), dim=1).cpu().numpy()[:, 1] \n",
    "            for model in training_models\n",
    "        ]\n",
    "        batch_probabilities = np.vstack(batch_probabilities)\n",
    "        probabilities.append(batch_probabilities)\n",
    "\n",
    "probabilities = np.hstack(probabilities)\n",
    "probabilities = np.mean(probabilities, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_df = pd.concat([test_df, new_test_df], ignore_index=True)\n",
    "probabilities = np.concatenate([test_labels, probabilities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "prediction = (probabilities >= threshold).astype(int)\n",
    "prediction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': all_test_df['id'], \n",
    "    'prediction': prediction\n",
    "})\n",
    "submission.to_csv('../submissions/multimodal.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_probs = pd.DataFrame({\n",
    "    'id': all_test_df['id'], \n",
    "    'probability': probabilities\n",
    "})\n",
    "submission_probs.to_csv('../submissions/multimodal_probs.csv', index=False)\n",
    "submission_probs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
