{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9c645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from vllm import LLM, SamplingParams\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    global SEED\n",
    "    SEED = seed\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text: str) -> str:\n",
    "    pattern = r\"FINAL\\s*ANSWER\\s*:\\s*([A-Z]+)\"\n",
    "    match = re.search(pattern, text.strip(), re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620e285",
   "metadata": {},
   "source": [
    "**Qwen 4B:**\n",
    "\n",
    "1) Qwen/Qwen3-4B-Instruct-2507 (8 GB)\n",
    "2) Qwen/Qwen3-4B-Thinking-2507 (8 GB)\n",
    "\n",
    "**Qwen 8B:**\n",
    "\n",
    "1) Qwen/Qwen3-8B (16 GB)\n",
    "\n",
    "**For math:**\n",
    "\n",
    "1) nvidia/OpenMath-Nemotron-7B (15 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    max_model_len=4096,\n",
    "    seed=SEED,\n",
    "    # allowed_local_media_path=os.path.abspath('images'), # for VLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d551a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.pickle', \"rb\") as input_file:\n",
    "    model_input = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbf7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params_confident = SamplingParams(\n",
    "    temperature=0.4,\n",
    "    top_p=0.9,\n",
    "    max_tokens=3600,\n",
    ") # самые уверенные параметры\n",
    "sampling_params_varied = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=3600,\n",
    ") # более разнообразные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "model_output = []\n",
    "\n",
    "prompts = []\n",
    "rids = []\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# ======================== Формируем запросы ===========================\n",
    "for row in tqdm(model_input):\n",
    "    rid = row[\"rid\"]\n",
    "    question = row[\"question\"]\n",
    "    image = Image.open(io.BytesIO(row[\"image\"]))\n",
    "    image_path = f\"images/{rid}.png\"\n",
    "    image.save(image_path)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Choose all the correct options:\\n{question}\\n\\n\n",
    "Write a detailed explanation and the correct answer as a sequence of letters you chose and format it as \n",
    "FINAL ANSWER: <Answer> (example: FINAL ANSWER: AD). Rely only on the information present in the picture.\n",
    "Check every fact very carefully and NEVER invent anything new or not related to the picture.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                # {\"type\": \"image_url\", \"image_url\": {\"url\": f\"file://{os.path.abspath(image_path)}\"}}, # FOR VLM\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompts.append(messages)\n",
    "    rids.append(rid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc804dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Инференс ===========================\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Inference\"):\n",
    "    batch_prompts = prompts[i:i + BATCH_SIZE]\n",
    "    batch_rids = rids[i:i + BATCH_SIZE]\n",
    "\n",
    "    all_answers = {rid: [] for rid in batch_rids}\n",
    "\n",
    "    # --- Прогон 1 (уверенный) ---\n",
    "    outputs1 = llm.chat(messages=batch_prompts, sampling_params=sampling_params_confident)\n",
    "    for rid, output in zip(batch_rids, outputs1):\n",
    "        text = output.outputs[0].text\n",
    "        all_answers[rid].append(parse(text) or \"B\")\n",
    "\n",
    "    # --- Прогоны 2 и 3 (вариативные) ---\n",
    "    for _ in range(2):\n",
    "        outputs = llm.chat(messages=batch_prompts, sampling_params=sampling_params_varied)\n",
    "        for rid, output in zip(batch_rids, outputs):\n",
    "            text = output.outputs[0].text\n",
    "            all_answers[rid].append(parse(text) or \"B\")\n",
    "\n",
    "    # --- Выбор финального ответа ---\n",
    "    for rid in batch_rids:\n",
    "        answers = [a for a in all_answers[rid] if a]\n",
    "        unique = set(answers)\n",
    "        if len(unique) == 3:\n",
    "            # все три разные → берем 1-й (уверенный)\n",
    "            final_answer = answers[0]\n",
    "        else:\n",
    "            # иначе мода\n",
    "            final_answer = Counter(answers).most_common(1)[0][0]\n",
    "        model_output.append({\"rid\": rid, \"answer\": final_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b89dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.json', 'w') as output_file:\n",
    "    json.dump(model_output, output_file, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
