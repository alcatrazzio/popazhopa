{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7638780",
   "metadata": {},
   "source": [
    "# POI Success Prediction - Enhanced Solution\n",
    "\n",
    "**Goal:** Predict POI success rating (1-5) using location, demographics, and reviews.\n",
    "\n",
    "**Strategy:**\n",
    "- Feature engineering: coordinates, geo clusters, density ratios, XLM-R embeddings, PCA\n",
    "- Target encoding within CV (no leakage)\n",
    "- Ensemble: CatBoost + LightGBM with tuned hyperparameters\n",
    "- 5-fold Stratified CV with MAE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e0c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892052a3",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7953394c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (41105, 286), Test: (9276, 285), Reviews: (440082, 2)\n",
      "Target distribution:\n",
      "target\n",
      "0.0    3938\n",
      "0.1      16\n",
      "0.2       3\n",
      "0.4       1\n",
      "0.5       1\n",
      "0.8      14\n",
      "0.9      13\n",
      "1.0      14\n",
      "1.1       7\n",
      "1.2       6\n",
      "1.3      21\n",
      "1.4      16\n",
      "1.5      16\n",
      "1.6      17\n",
      "1.7      25\n",
      "1.8      47\n",
      "1.9      43\n",
      "2.0      43\n",
      "2.1      61\n",
      "2.2      85\n",
      "2.3     111\n",
      "2.4      84\n",
      "2.5     101\n",
      "2.6     111\n",
      "2.7     106\n",
      "2.8     271\n",
      "2.9     207\n",
      "3.0     246\n",
      "3.1     346\n",
      "3.2     616\n",
      "3.3    1801\n",
      "3.4    2321\n",
      "3.5    2977\n",
      "3.6    4426\n",
      "3.7    4288\n",
      "3.8    3370\n",
      "3.9    3012\n",
      "4.0    2887\n",
      "4.1    2668\n",
      "4.2    2735\n",
      "4.3    1763\n",
      "4.4     880\n",
      "4.5     418\n",
      "4.6     275\n",
      "4.7     196\n",
      "4.8     225\n",
      "4.9     133\n",
      "5.0     144\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def read_tsv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, sep='\\t', encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, sep='\\t', encoding='cp1251')\n",
    "\n",
    "train = read_tsv('vseross/C/train.tsv')\n",
    "test = read_tsv('vseross/C/test.tsv')\n",
    "reviews = read_tsv('vseross/C/reviews.tsv')\n",
    "\n",
    "# Optional debug sampling\n",
    "debug_rows = os.environ.get('DEBUG_ROWS')\n",
    "if debug_rows is not None:\n",
    "    try:\n",
    "        n = int(debug_rows)\n",
    "        train = train.head(n).copy()\n",
    "        test = test.head(max(1, n // 2)).copy()\n",
    "        reviews = reviews[reviews['id'].isin(pd.concat([train['id'], test['id']]).unique())].copy()\n",
    "        print(f'DEBUG mode: train {len(train)}, test {len(test)}, reviews {len(reviews)}')\n",
    "    except Exception as e:\n",
    "        print('DEBUG_ROWS set but failed to sample:', e)\n",
    "\n",
    "print(f'Train: {train.shape}, Test: {test.shape}, Reviews: {reviews.shape}')\n",
    "print(f'Target distribution:\\n{train[\"target\"].value_counts().sort_index()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f987a19",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc422154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates parsed\n"
     ]
    }
   ],
   "source": [
    "# Parse coordinates\n",
    "for df in [train, test]:\n",
    "    coords = df['coordinates'].str.strip('[]').str.split(',', expand=True)\n",
    "    df['longitude'] = pd.to_numeric(coords[0].str.strip(), errors='coerce')\n",
    "    df['latitude'] = pd.to_numeric(coords[1].str.strip(), errors='coerce')\n",
    "    df['category'] = df['category'].fillna('unknown').astype(str)\n",
    "\n",
    "    df[\"lat_rad\"] = np.radians(df[\"latitude\"])\n",
    "    df[\"lon_rad\"] = np.radians(df[\"longitude\"])\n",
    "    df[\"sin_lat\"] = np.sin(df[\"lat_rad\"])\n",
    "    df[\"cos_lat\"] = np.cos(df[\"lat_rad\"])\n",
    "    df[\"sin_lon\"] = np.sin(df[\"lon_rad\"])\n",
    "    df[\"cos_lon\"] = np.cos(df[\"lon_rad\"])\n",
    "    df[\"lat_lon_ratio\"] = df[\"latitude\"] / (df[\"longitude\"] + 1e-6)\n",
    "    df[\"coord_density\"] = df.groupby([\"latitude\", \"longitude\"])[\"id\"].transform(\"count\")\n",
    "\n",
    "print('Coordinates parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36408fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name/address features created\n"
     ]
    }
   ],
   "source": [
    "# Name/address features and chain indicator\n",
    "both = pd.concat([train[['id', 'name']], test[['id', 'name']]])\n",
    "name_counts = both['name'].fillna('').astype(str).value_counts()\n",
    "train['name_count'] = train['name'].fillna('').astype(str).map(name_counts).fillna(1).astype(int)\n",
    "test['name_count'] = test['name'].fillna('').astype(str).map(name_counts).fillna(1).astype(int)\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['is_chain'] = (df['name_count'] > 1).astype(int)\n",
    "    df['name_len'] = df['name'].fillna('').astype(str).str.len()\n",
    "    df['name_words'] = df['name'].fillna('').astype(str).str.split().map(len)\n",
    "    df['has_digits_name'] = df['name'].fillna('').astype(str).str.contains(r'\\d').astype(int)\n",
    "    df['address_len'] = df['address'].fillna('').astype(str).str.len()\n",
    "    df['has_digits_address'] = df['address'].fillna('').astype(str).str.contains(r'\\d').astype(int)\n",
    "    # NEW: Extract brand/network from name (first word often brand)\n",
    "    df['name_first_word'] = df['name'].fillna('').astype(str).str.split().str[0].str.lower()\n",
    "\n",
    "print('Name/address features created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36774dd1",
   "metadata": {},
   "source": [
    "### Review Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761802c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6e5c306e6f4ffd932eba358d9dcc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review stats created\n"
     ]
    }
   ],
   "source": [
    "# Aggregate reviews: count, mean length, and sentiment\n",
    "review_stats = reviews.groupby('id').agg(\n",
    "    review_count=('text', 'count'),\n",
    "    review_len_mean=('text', lambda x: np.mean([len(str(t)) for t in x])),\n",
    "    review_len_std=('text', lambda x: np.std([len(str(t)) for t in x]))\n",
    ").reset_index()\n",
    "\n",
    "# Simple sentiment: count positive/negative words\n",
    "POS_WORDS = ['отлично', 'хорошо', 'рекомендую', 'понравилось', 'лучший', 'доброжелательный', \n",
    "             'вкусно', 'чисто', 'удобно', 'дешево', 'прекрасно', 'замечательно', 'отличный']\n",
    "NEG_WORDS = ['плохо', 'ужасно', 'не понравилось', 'дорого', 'грязно', 'хамство', 'разочарован', \n",
    "             'отвратительно', 'проблема', 'жалоба', 'ужасный', 'никогда', 'обман']\n",
    "\n",
    "def sentiment_score(text):\n",
    "    text = str(text).lower()\n",
    "    pos = sum(word in text for word in POS_WORDS)\n",
    "    neg = sum(word in text for word in NEG_WORDS)\n",
    "    return pos - neg\n",
    "\n",
    "ids_grouped = reviews.groupby('id')['text']\n",
    "sentiment = ids_grouped.progress_apply(lambda x: np.mean([sentiment_score(t) for t in x]))\n",
    "sentiment = sentiment.reset_index().rename(columns={'text': 'sentiment_score'})\n",
    "review_stats = review_stats.merge(sentiment, on='id', how='left')\n",
    "\n",
    "train = train.merge(review_stats, on='id', how='left')\n",
    "test = test.merge(review_stats, on='id', how='left')\n",
    "for col in ['review_count', 'review_len_mean', 'review_len_std', 'sentiment_score']:\n",
    "    train[col] = train[col].fillna(0)\n",
    "    test[col] = test[col].fillna(0)\n",
    "\n",
    "print('Review stats created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbc66e",
   "metadata": {},
   "source": [
    "### TF-IDF Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a115fe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d78c7fc6b54036ab854ce7500ff9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features: 32 dims\n"
     ]
    }
   ],
   "source": [
    "all_reviews = (\n",
    "    ids_grouped.progress_apply(lambda x: ' '.join(x))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Stronger text: bigrams and more features, reduce with SVD\n",
    "vectorizer = TfidfVectorizer(max_features=400, ngram_range=(1, 2), lowercase=True)\n",
    "X_tfidf = vectorizer.fit_transform(all_reviews['text'])\n",
    "svd = TruncatedSVD(n_components=32, random_state=42)\n",
    "X_svd = svd.fit_transform(X_tfidf)\n",
    "svd_df = pd.DataFrame(X_svd, columns=[f'tfidf_{i}' for i in range(X_svd.shape[1])])\n",
    "svd_df['id'] = all_reviews['id']\n",
    "train = train.merge(svd_df, on='id', how='left')\n",
    "test = test.merge(svd_df, on='id', how='left')\n",
    "for col in [c for c in train.columns if c.startswith('tfidf_')]:\n",
    "    train[col] = train[col].fillna(0)\n",
    "    test[col] = test[col].fillna(0)\n",
    "\n",
    "print(f'TF-IDF features: {X_svd.shape[1]} dims')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339efc6",
   "metadata": {},
   "source": [
    "### XLM-RoBERTa Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f297d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding XLM-R embeddings using sergeyzh/BERTA ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default prompt name is set to 'Classification'. This prompt will be applied to all `encode()` calls, except if `encode()` is called with `prompt` or `prompt_name` parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7b4b86b10d49bb82d323b7bf73bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing XLM-R embeddings from 768 to 64 via SVD\n",
      "XLM-R embeddings: 64 dims\n"
     ]
    }
   ],
   "source": [
    "USE_XLMR = os.environ.get('USE_XLMR', '1') != '0'\n",
    "if USE_XLMR and len(reviews) > 0:\n",
    "    MAX_REVIEWS_PER_ID = int(os.environ.get('MAX_REVIEWS_PER_ID', '50'))\n",
    "    MAX_CHARS = int(os.environ.get('MAX_REVIEW_CHARS', '2000'))\n",
    "    model_name = 'sergeyzh/BERTA'#os.environ.get('XLMR_MODEL', 'paraphrase-xlm-r-multilingual-v1')\n",
    "    batch_size = int(os.environ.get('XLMR_BATCH', '32'))\n",
    "    n_comp = int(os.environ.get('XLMR_SVD', '64'))\n",
    "\n",
    "    agg_reviews = (\n",
    "        reviews.groupby('id')['text']\n",
    "        .apply(lambda x: ' '.join(list(x)[:MAX_REVIEWS_PER_ID])[:MAX_CHARS])\n",
    "        .reset_index()\n",
    "        .rename(columns={'text': 'agg_text'})\n",
    "    )\n",
    "\n",
    "    print(f'Encoding XLM-R embeddings using {model_name} ...')\n",
    "    st_model = SentenceTransformer(model_name)\n",
    "    emb = st_model.encode(\n",
    "        agg_reviews['agg_text'].tolist(),\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=False\n",
    "    )\n",
    "\n",
    "    if n_comp > 0 and emb.shape[1] > n_comp:\n",
    "        print(f'Reducing XLM-R embeddings from {emb.shape[1]} to {n_comp} via SVD')\n",
    "        svd_x = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "        emb = svd_x.fit_transform(emb)\n",
    "\n",
    "    # Create column names based on embedding dimensions\n",
    "    xlmr_cols = [f'xlmr_{i}' for i in range(emb.shape[1])]\n",
    "    xlmr_df = pd.DataFrame(emb, columns=xlmr_cols)\n",
    "    xlmr_df['id'] = agg_reviews['id'].values\n",
    "    train = train.merge(xlmr_df, on='id', how='left')\n",
    "    test = test.merge(xlmr_df, on='id', how='left')\n",
    "    for c in xlmr_cols:\n",
    "        train[c] = train[c].fillna(0)\n",
    "        test[c] = test[c].fillna(0)\n",
    "    print(f'XLM-R embeddings: {len(xlmr_cols)} dims')\n",
    "else:\n",
    "    print('XLM-R embeddings skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7985fb",
   "metadata": {},
   "source": [
    "### Geo Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d52947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans clusters: 80\n"
     ]
    }
   ],
   "source": [
    "# Geo clusters from coordinates\n",
    "coords_all = pd.concat([\n",
    "    train[['longitude', 'latitude']],\n",
    "    test[['longitude', 'latitude']]\n",
    "], axis=0).reset_index(drop=True)\n",
    "\n",
    "coords_fit = coords_all.dropna()\n",
    "if len(coords_fit) > 0:\n",
    "    # Dynamic cluster count based on dataset size\n",
    "    try:\n",
    "        n_clusters = int(np.clip(np.sqrt(len(coords_fit) / 2), 20, 80))\n",
    "    except Exception:\n",
    "        n_clusters = 50\n",
    "    n_clusters = max(2, min(n_clusters, len(coords_fit)))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(coords_fit[['longitude', 'latitude']])\n",
    "    \n",
    "    def predict_clusters(df_xy: pd.DataFrame) -> np.ndarray:\n",
    "        xy = df_xy[['longitude', 'latitude']].copy()\n",
    "        xy['longitude'] = xy['longitude'].fillna(coords_fit['longitude'].mean())\n",
    "        xy['latitude'] = xy['latitude'].fillna(coords_fit['latitude'].mean())\n",
    "        return kmeans.predict(xy)\n",
    "    \n",
    "    train['kmeans_cluster'] = predict_clusters(train)\n",
    "    test['kmeans_cluster'] = predict_clusters(test)\n",
    "else:\n",
    "    train['kmeans_cluster'] = -1\n",
    "    test['kmeans_cluster'] = -1\n",
    "\n",
    "print(f'KMeans clusters: {n_clusters if len(coords_fit) > 0 else 0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b306721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance features created\n"
     ]
    }
   ],
   "source": [
    "# Distances: to global median center and to cluster center\n",
    "if len(coords_fit) > 0:\n",
    "    center_lon = coords_fit['longitude'].median()\n",
    "    center_lat = coords_fit['latitude'].median()\n",
    "    \n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        R = 6371.0\n",
    "        lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "        return R * c\n",
    "    \n",
    "    for df in (train, test):\n",
    "        df['dist_to_center_km'] = haversine(df['longitude'], df['latitude'], center_lon, center_lat)\n",
    "    \n",
    "    if 'kmeans_cluster' in train.columns:\n",
    "        centers = kmeans.cluster_centers_ if 'kmeans' in locals() else None\n",
    "        if centers is not None:\n",
    "            def dist_to_cluster(df):\n",
    "                idx = df['kmeans_cluster'].fillna(-1).astype(int).values\n",
    "                idx = np.where((idx >= 0) & (idx < len(centers)), idx, 0)\n",
    "                c_lon = centers[idx, 0]\n",
    "                c_lat = centers[idx, 1]\n",
    "                return haversine(df['longitude'].values, df['latitude'].values, c_lon, c_lat)\n",
    "            train['dist_to_kmeans_km'] = dist_to_cluster(train)\n",
    "            test['dist_to_kmeans_km'] = dist_to_cluster(test)\n",
    "\n",
    "print('Distance features created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9b575",
   "metadata": {},
   "source": [
    "### Density Ratios & Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63dd5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96464d052f394cae9b83ec37ab43e818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Density ratios:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d7c2399f5e4584ab7ecff0902f2187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Log transform:   0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density ratios and log transform applied\n"
     ]
    }
   ],
   "source": [
    "# Density ratios between 300m and 1000m rings\n",
    "def add_density_ratios(tr: pd.DataFrame, te: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    cols_300 = [c for c in tr.columns if c.endswith('_300m')]\n",
    "    for c300 in tqdm(cols_300, desc='Density ratios'):\n",
    "        base = c300[:-5]\n",
    "        c1000 = base + '_1000m'\n",
    "        if c1000 in tr.columns:\n",
    "            for df in (tr, te):\n",
    "                num = pd.to_numeric(df[c300], errors='coerce').fillna(0.0)\n",
    "                den = pd.to_numeric(df[c1000], errors='coerce').fillna(0.0)\n",
    "                df[f'{base}_ratio_300_1000'] = (num / (den + 1e-6)).astype(float)\n",
    "                df[f'{base}_perimeter_1000_only'] = (den - num).clip(lower=0)\n",
    "    return tr, te\n",
    "\n",
    "train, test = add_density_ratios(train, test)\n",
    "\n",
    "# Log-transform skewed features\n",
    "skewed = [c for c in train.columns if re.search(r'_(300m|1000m)$', c)]\n",
    "for col in tqdm(skewed, desc='Log transform'):\n",
    "    train[col] = np.log1p(train[col].clip(lower=0))\n",
    "    test[col] = np.log1p(test[col].clip(lower=0))\n",
    "\n",
    "print('Density ratios and log transform applied')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c61e5d",
   "metadata": {},
   "source": [
    "### PCA on Dense Demographic Features (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "479c05c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA features: 20 components, explained variance: 0.993\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to capture latent demographic patterns\n",
    "USE_PCA = True\n",
    "if USE_PCA:\n",
    "    demo_cols = [c for c in train.columns if re.search(r'_(300m|1000m)$', c)]\n",
    "    # Take a subset to speed up\n",
    "    demo_subset = demo_cols[:100] if len(demo_cols) > 100 else demo_cols\n",
    "    \n",
    "    pca = PCA(n_components=20, random_state=42)\n",
    "    X_pca_train = pca.fit_transform(train[demo_subset].fillna(0))\n",
    "    X_pca_test = pca.transform(test[demo_subset].fillna(0))\n",
    "    \n",
    "    pca_cols = [f'pca_demo_{i}' for i in range(X_pca_train.shape[1])]\n",
    "    train[pca_cols] = X_pca_train\n",
    "    test[pca_cols] = X_pca_test\n",
    "    print(f'PCA features: {len(pca_cols)} components, explained variance: {pca.explained_variance_ratio_.sum():.3f}')\n",
    "else:\n",
    "    print('PCA skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5824f6",
   "metadata": {},
   "source": [
    "## 3. Prepare Features & Target Encoding Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8393b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 702\n",
      "Categorical features: ['category', 'kmeans_cluster', 'name_first_word']\n"
     ]
    }
   ],
   "source": [
    "# Select features: keep 'category' as categorical\n",
    "exclude = ['id', 'name', 'address', 'coordinates', 'target']\n",
    "features = [c for c in train.columns if c not in exclude]\n",
    "cat_features = []\n",
    "if 'category' in features:\n",
    "    cat_features.append('category')\n",
    "if 'kmeans_cluster' in features:\n",
    "    cat_features.append('kmeans_cluster')\n",
    "if 'name_first_word' in features:\n",
    "    cat_features.append('name_first_word')\n",
    "\n",
    "print(f'Total features: {len(features)}')\n",
    "print(f'Categorical features: {cat_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9989bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target encoding helper inside CV for selected categoricals (smoothed)\n",
    "def target_encode(train_df: pd.DataFrame, y_tr: pd.Series, val_df: pd.DataFrame, \n",
    "                  test_df: pd.DataFrame, cols, alpha=20.0):\n",
    "    prior = y_tr.mean()\n",
    "    tr_out = train_df.copy()\n",
    "    val_out = val_df.copy()\n",
    "    te_out = test_df.copy()\n",
    "    for col in cols:\n",
    "        stats = y_tr.groupby(train_df[col]).agg(['mean', 'count'])\n",
    "        m = stats['mean']\n",
    "        c = stats['count']\n",
    "        smooth = (m * c + prior * alpha) / (c + alpha)\n",
    "        tr_out[f'te_{col}'] = train_df[col].map(smooth)\n",
    "        val_out[f'te_{col}'] = val_df[col].map(smooth).fillna(prior)\n",
    "        te_out[f'te_{col}'] = test_df[col].map(smooth).fillna(prior)\n",
    "    return tr_out, val_out, te_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7dc90",
   "metadata": {},
   "source": [
    "## 4. Train Ensemble Model (CatBoost + LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e34af8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-fold CV with CatBoost + LightGBM ensemble...\n"
     ]
    }
   ],
   "source": [
    "# Train with 5-fold Stratified CV (MAE)\n",
    "mask = train['target'] > 0\n",
    "X_all = train.loc[mask, features].copy()\n",
    "y_all = train.loc[mask, 'target'].copy()\n",
    "X_test_base = test[features].copy()\n",
    "\n",
    "# Stratification bins for regression stability\n",
    "bins = pd.qcut(y_all, q=10, labels=False, duplicates='drop')\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_cat = np.zeros(len(X_all))\n",
    "oof_lgb = np.zeros(len(X_all))\n",
    "test_preds_cat = np.zeros(len(test))\n",
    "test_preds_lgb = np.zeros(len(test))\n",
    "fold_maes_cat = []\n",
    "fold_maes_lgb = []\n",
    "\n",
    "splits = list(skf.split(X_all, bins))\n",
    "print(f'Starting {len(splits)}-fold CV with CatBoost + LightGBM ensemble...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543558f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f227d79edc84e48a6e93f2388666d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CV folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FOLD 1/5\n",
      "============================================================\n",
      "\n",
      "[Fold 1] Training CatBoost...\n",
      "[Fold 1] CatBoost MAE: 0.26101 (iterations: 2943)\n",
      "[Fold 1] Training LightGBM...\n",
      "[Fold 1] LightGBM MAE: 0.26609\n",
      "\n",
      "[Fold 1] Summary: CatBoost=0.26101, LightGBM=0.26609\n",
      "\n",
      "============================================================\n",
      "FOLD 2/5\n",
      "============================================================\n",
      "\n",
      "[Fold 2] Training CatBoost...\n",
      "[Fold 2] CatBoost MAE: 0.26383 (iterations: 2951)\n",
      "[Fold 2] Training LightGBM...\n",
      "[Fold 2] LightGBM MAE: 0.26440\n",
      "\n",
      "[Fold 2] Summary: CatBoost=0.26383, LightGBM=0.26440\n",
      "\n",
      "============================================================\n",
      "FOLD 3/5\n",
      "============================================================\n",
      "\n",
      "[Fold 3] Training CatBoost...\n",
      "[Fold 3] CatBoost MAE: 0.26294 (iterations: 2867)\n",
      "[Fold 3] Training LightGBM...\n",
      "[Fold 3] LightGBM MAE: 0.26568\n",
      "\n",
      "[Fold 3] Summary: CatBoost=0.26294, LightGBM=0.26568\n",
      "\n",
      "============================================================\n",
      "FOLD 4/5\n",
      "============================================================\n",
      "\n",
      "[Fold 4] Training CatBoost...\n",
      "[Fold 4] CatBoost MAE: 0.25992 (iterations: 1355)\n",
      "[Fold 4] Training LightGBM...\n",
      "[Fold 4] LightGBM MAE: 0.26041\n",
      "\n",
      "[Fold 4] Summary: CatBoost=0.25992, LightGBM=0.26041\n",
      "\n",
      "============================================================\n",
      "FOLD 5/5\n",
      "============================================================\n",
      "\n",
      "[Fold 5] Training CatBoost...\n",
      "[Fold 5] CatBoost MAE: 0.26090 (iterations: 2024)\n",
      "[Fold 5] Training LightGBM...\n",
      "[Fold 5] LightGBM MAE: 0.26277\n",
      "\n",
      "[Fold 5] Summary: CatBoost=0.26090, LightGBM=0.26277\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "\n",
      "=== CatBoost ===\n",
      "OOF MAE: 0.26172\n",
      "Fold MAEs: ['0.26101', '0.26383', '0.26294', '0.25992', '0.26090']\n",
      "Mean: 0.26172 ± 0.00144\n",
      "\n",
      "=== LightGBM ===\n",
      "OOF MAE: 0.26387\n",
      "Fold MAEs: ['0.26609', '0.26440', '0.26568', '0.26041', '0.26277']\n",
      "Mean: 0.26387 ± 0.00208\n"
     ]
    }
   ],
   "source": [
    "for fold, (tr_idx, va_idx) in enumerate(tqdm(splits, total=len(splits), desc='CV folds'), 1):\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'FOLD {fold}/{len(splits)}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    X_tr, X_val = X_all.iloc[tr_idx].copy(), X_all.iloc[va_idx].copy()\n",
    "    y_tr, y_val = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    # Target encoding on selected categorical cols\n",
    "    te_cols = []\n",
    "    if 'category' in X_tr.columns:\n",
    "        te_cols.append('category')\n",
    "    if 'kmeans_cluster' in X_tr.columns:\n",
    "        te_cols.append('kmeans_cluster')\n",
    "    if 'is_chain' in X_tr.columns:\n",
    "        te_cols.append('is_chain')\n",
    "    if 'name_first_word' in X_tr.columns:\n",
    "        te_cols.append('name_first_word')\n",
    "    \n",
    "    X_tr_te, X_val_te, X_test_te = target_encode(X_tr, y_tr, X_val, X_test_base, te_cols, alpha=30.0)\n",
    "\n",
    "    # --- CatBoost ---\n",
    "    print(f'\\n[Fold {fold}] Training CatBoost...')\n",
    "    model_cat = CatBoostRegressor(\n",
    "        iterations=3000,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_state=42 + fold,\n",
    "        l2_leaf_reg=3.0,\n",
    "        subsample=0.8,\n",
    "        bootstrap_type='Bernoulli',\n",
    "        verbose=False,\n",
    "        task_type='CPU',\n",
    "        thread_count=-1\n",
    "    )\n",
    "    model_cat.fit(\n",
    "        X_tr_te, y_tr,\n",
    "        eval_set=(X_val_te, y_val),\n",
    "        use_best_model=True,\n",
    "        early_stopping_rounds=300,\n",
    "        cat_features=cat_features,\n",
    "        verbose=False\n",
    "    )\n",
    "    val_pred_cat = model_cat.predict(X_val_te)\n",
    "    oof_cat[va_idx] = val_pred_cat\n",
    "    fold_mae_cat = mean_absolute_error(y_val, val_pred_cat)\n",
    "    fold_maes_cat.append(fold_mae_cat)\n",
    "    test_preds_cat += model_cat.predict(X_test_te) / skf.n_splits\n",
    "    print(f'[Fold {fold}] CatBoost MAE: {fold_mae_cat:.5f} (iterations: {model_cat.best_iteration_})')\n",
    "\n",
    "    # --- LightGBM ---\n",
    "    # Convert object columns to numeric for LightGBM\n",
    "    X_tr_te_lgb = X_tr_te.copy()\n",
    "    X_val_te_lgb = X_val_te.copy()\n",
    "    X_test_te_lgb = X_test_te.copy()\n",
    "    \n",
    "    for col in cat_features:\n",
    "        if col in X_tr_te_lgb.columns and X_tr_te_lgb[col].dtype == 'object':\n",
    "            # Convert to category codes\n",
    "            all_vals = pd.concat([X_tr_te_lgb[col], X_val_te_lgb[col], X_test_te_lgb[col]]).astype(str)\n",
    "            categories = all_vals.unique()\n",
    "            cat_map = {cat: i for i, cat in enumerate(categories)}\n",
    "            \n",
    "            X_tr_te_lgb[col] = X_tr_te_lgb[col].astype(str).map(cat_map).fillna(-1).astype(int)\n",
    "            X_val_te_lgb[col] = X_val_te_lgb[col].astype(str).map(cat_map).fillna(-1).astype(int)\n",
    "            X_test_te_lgb[col] = X_test_te_lgb[col].astype(str).map(cat_map).fillna(-1).astype(int)\n",
    "    \n",
    "    print(f'[Fold {fold}] Training LightGBM...')\n",
    "    model_lgb = LGBMRegressor(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=64,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=1.5,\n",
    "        random_state=42 + fold,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model_lgb.fit(\n",
    "        X_tr_te_lgb, y_tr,\n",
    "        eval_set=[(X_val_te_lgb, y_val)],\n",
    "        eval_metric='mae'\n",
    "    )\n",
    "    val_pred_lgb = model_lgb.predict(X_val_te_lgb)\n",
    "    oof_lgb[va_idx] = val_pred_lgb\n",
    "    fold_mae_lgb = mean_absolute_error(y_val, val_pred_lgb)\n",
    "    fold_maes_lgb.append(fold_mae_lgb)\n",
    "    test_preds_lgb += model_lgb.predict(X_test_te_lgb) / skf.n_splits\n",
    "    print(f'[Fold {fold}] LightGBM MAE: {fold_mae_lgb:.5f}')\n",
    "    \n",
    "    print(f'\\n[Fold {fold}] Summary: CatBoost={fold_mae_cat:.5f}, LightGBM={fold_mae_lgb:.5f}')\n",
    "\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print('FINAL RESULTS')\n",
    "print(f'{\"=\"*60}')\n",
    "\n",
    "print('\\n=== CatBoost ===')\n",
    "print(f'OOF MAE: {mean_absolute_error(y_all, oof_cat):.5f}')\n",
    "print(f'Fold MAEs: {[f\"{m:.5f}\" for m in fold_maes_cat]}')\n",
    "print(f'Mean: {np.mean(fold_maes_cat):.5f} ± {np.std(fold_maes_cat):.5f}')\n",
    "\n",
    "print('\\n=== LightGBM ===')\n",
    "print(f'OOF MAE: {mean_absolute_error(y_all, oof_lgb):.5f}')\n",
    "print(f'Fold MAEs: {[f\"{m:.5f}\" for m in fold_maes_lgb]}')\n",
    "print(f'Mean: {np.mean(fold_maes_lgb):.5f} ± {np.std(fold_maes_lgb):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d9118a",
   "metadata": {},
   "source": [
    "## 5. Ensemble & Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f17d2c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal ensemble weight (CatBoost): 0.55\n",
      "Ensemble OOF MAE: 0.25795\n"
     ]
    }
   ],
   "source": [
    "# Weighted ensemble: blend CatBoost and LightGBM\n",
    "# Find optimal weight on OOF\n",
    "best_weight = 0.5\n",
    "best_mae = float('inf')\n",
    "for w in np.linspace(0, 1, 21):\n",
    "    oof_blend = w * oof_cat + (1 - w) * oof_lgb\n",
    "    mae = mean_absolute_error(y_all, oof_blend)\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_weight = w\n",
    "\n",
    "print(f'\\nOptimal ensemble weight (CatBoost): {best_weight:.2f}')\n",
    "print(f'Ensemble OOF MAE: {best_mae:.5f}')\n",
    "\n",
    "# Apply ensemble to test predictions\n",
    "pred = best_weight * test_preds_cat + (1 - best_weight) * test_preds_lgb\n",
    "pred = np.clip(pred, 1, 5)\n",
    "\n",
    "submission = pd.DataFrame({'id': test['id'], 'target': pred})\n",
    "submission.to_csv('mellstroy.game.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "198d12c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.932949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.386047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>3.857323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.372389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>3.226973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>30097</td>\n",
       "      <td>3.540172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>21993</td>\n",
       "      <td>3.938983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>43919</td>\n",
       "      <td>3.983552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>46598</td>\n",
       "      <td>3.561007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275</th>\n",
       "      <td>29393</td>\n",
       "      <td>3.895240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9276 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    target\n",
       "0     21472  3.932949\n",
       "1      9837  3.386047\n",
       "2     41791  3.857323\n",
       "3     18441  3.372389\n",
       "4     49348  3.226973\n",
       "...     ...       ...\n",
       "9271  30097  3.540172\n",
       "9272  21993  3.938983\n",
       "9273  43919  3.983552\n",
       "9274  46598  3.561007\n",
       "9275  29393  3.895240\n",
       "\n",
       "[9276 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d24d5a",
   "metadata": {},
   "source": [
    "# Automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a12ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b183c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = X_all.copy()\n",
    "full_df['target'] = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95a7cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20251021_155055\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          12\n",
      "Memory Avail:       46.51 GB / 63.46 GB (73.3%)\n",
      "Disk Space Avail:   179.70 GB / 879.97 GB (20.4%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"C:\\Projects\\aiijc\\AutogluonModels\\ag-20251021_155055\"\n",
      "Train Data Rows:    37167\n",
      "Train Data Columns: 702\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and label-values can't be converted to int).\n",
      "\tLabel info (max, min, mean, stddev): (5.0, 0.1, 3.75986, 0.46157)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    47621.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 194.60 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 690 | ['traffic_300m', 'homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', ...]\n",
      "\t\t('int', [])    :  10 | ['coord_density', 'name_count', 'is_chain', 'name_len', 'name_words', ...]\n",
      "\t\t('object', []) :   2 | ['category', 'name_first_word']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   2 | ['category', 'name_first_word']\n",
      "\t\t('float', [])     : 690 | ['traffic_300m', 'homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', ...]\n",
      "\t\t('int', [])       :   7 | ['coord_density', 'name_count', 'name_len', 'name_words', 'address_len', ...]\n",
      "\t\t('int', ['bool']) :   3 | ['is_chain', 'has_digits_name', 'has_digits_address']\n",
      "\t2.7s = Fit runtime\n",
      "\t702 features in original data used to generate 702 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 188.50 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.92s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.06726397072671994, Train Rows: 34667, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Excluded models: ['RF'] (Specified by `excluded_model_types`)\n",
      "Fitting 8 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 897.08s of the 897.08s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.0/46.1 GB\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l1: 0.232579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.2325\t = Validation score   (-mean_absolute_error)\n",
      "\t38.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 858.35s of the 858.35s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.0/46.7 GB\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.2339\t = Validation score   (-mean_absolute_error)\n",
      "\t9.54s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 848.77s of the 848.77s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.9/46.7 GB\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because MAE is/are not implemented for GPU\n",
      "Default metric period is 5 because MAE is/are not implemented for GPU\n",
      "\t-0.2317\t = Validation score   (-mean_absolute_error)\n",
      "\t38.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 810.40s of the 810.40s of remaining time.\n",
      "\tFitting with cpus=12, gpus=1, mem=0.0/46.6 GB\n",
      "\t-0.2444\t = Validation score   (-mean_absolute_error)\n",
      "\t164.2s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 645.89s of the 645.89s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.7/45.3 GB\n",
      "No improvement since epoch 6: early stopping\n",
      "\t-0.2509\t = Validation score   (-mean_absolute_error)\n",
      "\t29.01s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 616.78s of the 616.78s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.5/45.1 GB\n",
      "\t-0.2393\t = Validation score   (-mean_absolute_error)\n",
      "\t8.19s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 608.48s of the 608.48s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=0.9/44.9 GB\n",
      "\t-0.2428\t = Validation score   (-mean_absolute_error)\n",
      "\t42.06s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 566.13s of the 566.13s of remaining time.\n",
      "\tFitting with cpus=6, gpus=1, mem=1.4/44.7 GB\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.2332\t = Validation score   (-mean_absolute_error)\n",
      "\t42.61s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 523.37s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.304, 'LightGBMLarge': 0.217, 'LightGBMXT': 0.174, 'NeuralNetFastAI': 0.13, 'XGBoost': 0.087, 'NeuralNetTorch': 0.087}\n",
      "\t-0.2264\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 377.09s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4915.9 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Projects\\aiijc\\AutogluonModels\\ag-20251021_155055\")\n"
     ]
    }
   ],
   "source": [
    "train_data = TabularDataset(full_df)\n",
    "predictor = TabularPredictor(\n",
    "    label='target', \n",
    "    eval_metric='mae'\n",
    ").fit(\n",
    "    train_data,\n",
    "    time_limit=900,        # тайм-лимит в секундах\n",
    "    presets='medium_quality',  # можно 'high_quality', 'medium_quality', 'fast_train'\n",
    "    ag_args_fit={'num_gpus': 1},  # используем GPU (1 GPU)\n",
    "    excluded_model_types=['RF']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ace5f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_sub = submission.copy()\n",
    "test_data = TabularDataset(X_test_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69a60246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.723485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.352265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>4.197441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.405224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>3.263135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>30097</td>\n",
       "      <td>3.103590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>21993</td>\n",
       "      <td>3.968985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>43919</td>\n",
       "      <td>3.977589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>46598</td>\n",
       "      <td>3.586091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275</th>\n",
       "      <td>29393</td>\n",
       "      <td>3.894850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9276 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    target\n",
       "0     21472  3.723485\n",
       "1      9837  3.352265\n",
       "2     41791  4.197441\n",
       "3     18441  3.405224\n",
       "4     49348  3.263135\n",
       "...     ...       ...\n",
       "9271  30097  3.103590\n",
       "9272  21993  3.968985\n",
       "9273  43919  3.977589\n",
       "9274  46598  3.586091\n",
       "9275  29393  3.894850\n",
       "\n",
       "[9276 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl_sub['target'] = predictor.predict(test_data)\n",
    "automl_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb49ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_sub.to_csv('automl_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98a4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
